---
tags:
  - independence
  - probability
  - measure
---
In basic probability, independence between two events $A$ and $B$ is defined as $P(A)P(B)=P(A\cap B)$. 

We can broad the definition to consider $\sigma$-algebras. Two $\sigma$-algebras $\mathcal{H},\mathcal{G}\subset\mathcal{F}$ are independent if
$$P(G\cap H)=P(G)P(H)\quad\forall G\in \mathcal{G},H\in\mathcal{H}$$

Two $\sigma$-algebras are independent if every element of one is independent of the other.

The random vectors $X$ and $Y$ on the same probability space are *independent* if the corresponding $\sigma$-algebras $\sigma(X)$ and $\sigma(Y)$ are independent.

The pairwise independence can be extended to the stronger notion of [[Mutual Independence (Measure)|mutual independence]].